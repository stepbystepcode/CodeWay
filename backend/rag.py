import dotenv

dotenv.load_dotenv()
from langchain_openai import ChatOpenAI

chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
data = loader.load()
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
# k is the number of chunks to retrieve
retriever = vectorstore.as_retriever(k=4)

docs = retriever.invoke("Can LangSmith help test my LLM applications?")
print(docs)
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# SYSTEM_TEMPLATE = """
# Answer the user's questions based on the below context. 
# If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":

# <context>
# {context}
# </context>
# """

# question_answering_prompt = ChatPromptTemplate.from_messages(
#     [
#         (
#             "system",
#             SYSTEM_TEMPLATE,
#         ),
#         MessagesPlaceholder(variable_name="messages"),
#     ]
# )

# document_chain = create_stuff_documents_chain(chat, question_answering_prompt)
# from langchain_core.messages import HumanMessage

# document_chain.invoke(
#     {
#         "context": docs,
#         "messages": [
#             HumanMessage(content="Can LangSmith help test my LLM applications?")
#         ],
#     }
# )
# document_chain.invoke(
#     {
#         "context": [],
#         "messages": [
#             HumanMessage(content="Can LangSmith help test my LLM applications?")
#         ],
#     }
# )
# from typing import Dict

# from langchain_core.runnables import RunnablePassthrough


# def parse_retriever_input(params: Dict):
#     return params["messages"][-1].content


# retrieval_chain = RunnablePassthrough.assign(
#     context=parse_retriever_input | retriever,
# ).assign(
#     answer=document_chain,
# )
# retrieval_chain.invoke(
#     {
#         "messages": [
#             HumanMessage(content="Can LangSmith help test my LLM applications?")
#         ],
#     }
# )